{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a022fd7c",
   "metadata": {},
   "source": [
    "# Main File for Deduplication Grouping / Processing / Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29af497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "from geopy.distance import geodesic\n",
    "# !pip install rapidfuzz\n",
    "from rapidfuzz import process,fuzz\n",
    "from rapidfuzz.fuzz import partial_ratio\n",
    "from shapely.geometry import Point\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('./deduplication'))\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"data\")))\n",
    "from data.data_setup import generate_df_from_bb\n",
    "from geohash_utils import assign_geohashes, get_neighboring_geohashes\n",
    "from name_utils import NYC_BLACKLIST, remove_common_words, clean_name, choose_common_name_from_group\n",
    "from other_utils import select_most_recent_row, extract_top_category, save_results_to_gdf, calculate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c028bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Filtering based on Fuzzy Matching + Spatial Promixity\n",
    "def group_similar_names_spatial_graph(df, tree, coords, earth_radius, max_distance, similarity_threshold=85, blacklist = NYC_BLACKLIST):\n",
    "    \"\"\" \n",
    "    Parameters:\n",
    "        df (DataFrame): FSQ DataFrame filtered for a specific category of POIs in a single geohash grid\n",
    "    \n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # adding all POIs as nodes\n",
    "    for _, row in df.iterrows():\n",
    "        G.add_node(row['fsq_place_id'], name = row['name'], lon = row['longitude'], lat = row['latitude'])\n",
    "\n",
    "    # build edges based on spatial proximity and name similarity\n",
    "    for i in range(len(df)):\n",
    "        # if i not in visited:\n",
    "        fsq_id_i = df.iloc[i]['fsq_place_id']\n",
    "        name_i = df.iloc[i]['name']\n",
    "        indices = tree.query_radius([coords[i]], r=max_distance / earth_radius)[0]\n",
    "\n",
    "        for j in indices:\n",
    "            if j != i:\n",
    "                fsq_id_j = df.iloc[j]['fsq_place_id']\n",
    "                name_j = df.iloc[j]['name']\n",
    "            # Calculate similarity using token sort ratio\n",
    "                sim_i_j = fuzz.token_set_ratio(remove_common_words(clean_name(name_i), blacklist, False), remove_common_words(clean_name(name_j), blacklist, False))\n",
    "                if sim_i_j >= similarity_threshold:\n",
    "                    G.add_edge(fsq_id_i, fsq_id_j)\n",
    "\n",
    "    # Find connected components\n",
    "    groups = [g for g in list(nx.connected_components(G)) if len(g) > 1]\n",
    "    return groups     \n",
    "\n",
    "### Step 3: Process a Geohash Group\n",
    "# process geohash groups\n",
    "async def process_groups(gdf, hash, category_list, name_similarity_threshold, max_distance, precision=7, blacklist = NYC_BLACKLIST, resolved_map = {}, file_path = None):\n",
    "    # Ensure gdf is in WGS84 for geohashing\n",
    "    gdf = gdf.to_crs(epsg=4326)  \n",
    "    # collect pois in the geohash and its neighbors\n",
    "    neighboring_geohashes = get_neighboring_geohashes(hash)\n",
    "    local_gdf = gdf[gdf['geohash'].isin(neighboring_geohashes)].copy()\n",
    "    earth_radius = 6371000  # Earth's radius (m)\n",
    "\n",
    "    all_group_metrics = []\n",
    "    # iterate over each category and update df to remove duplicates\n",
    "    for category in category_list:\n",
    "        # print(\"category: \"+category)\n",
    "        category_gdf = local_gdf[local_gdf['top_category'] == category].copy()\n",
    "        if category_gdf.empty:\n",
    "            continue\n",
    "\n",
    "        coords = np.radians(np.array(list(zip(category_gdf.geometry.y, category_gdf.geometry.x))))\n",
    "        tree = BallTree(coords, metric='haversine')\n",
    "  \n",
    "        # create groups of indices of POIs with similar names within a certain distance in the geohash\n",
    "        sim_name_close_groups = group_similar_names_spatial_graph(category_gdf, tree, coords, earth_radius, max_distance, name_similarity_threshold, blacklist)\n",
    "        # print(\"sim_name_close_groups: \" + str(sim_name_close_groups)) if sim_name_close_groups else print(\"No groups\")\n",
    "        # input parent ids for each group in sim_name_close_groups\n",
    "        # parent_ids, group_parent_dict, df_w_parent = await assign_parent_ids(category_gdf, sim_name_close_groups)\n",
    "\n",
    "        if sim_name_close_groups:        \n",
    "            # for each group of POIs with similar names,\n",
    "            # if the group contains 2+ POIs with the same parent id, combine them by centroid and keep the most frequent name\n",
    "            # if the group does not have a parent id, keep the most recent POI based on 'date_refreshed' or 'date_created'\n",
    "            for group in sim_name_close_groups:\n",
    "                if len(group) > 1:\n",
    "                    print(\"\\nGroup:\")\n",
    "                    print(category_gdf[category_gdf['fsq_place_id'].isin(group)][['name']])\n",
    "                    # print(\"addresses: \" + category_gdf[category_gdf['fsq_place_id'].isin(group)][['address']])\n",
    "                    \n",
    "                    rows_in_group = category_gdf[category_gdf['fsq_place_id'].isin(group)]\n",
    "                    latitude = rows_in_group['latitude'].mean()\n",
    "                    longitude = rows_in_group['longitude'].mean()\n",
    "                    centroid = Point(longitude, latitude)\n",
    "\n",
    "                    # metrics\n",
    "                    all_group_metrics.append(calculate_metrics(rows_in_group, group, category, centroid))\n",
    "                    names = [n for n in rows_in_group['name'].tolist()]\n",
    "                    \n",
    "                    # common_name = longest_common_substring(names) \n",
    "                    common_name = choose_common_name_from_group(names, blacklist)\n",
    "                    if common_name is None:\n",
    "                        print(\"No common name found for group, skipping...\")\n",
    "                        continue\n",
    "                        \n",
    "                    most_recent_row = select_most_recent_row(rows_in_group)\n",
    "                    print(\"most recent name: \"+ most_recent_row['name'])\n",
    "\n",
    "                    # id to keep\n",
    "                    most_recent_row_id = most_recent_row['fsq_place_id'] if isinstance(most_recent_row, pd.Series) else most_recent_row.iloc[0]['fsq_place_id']\n",
    "\n",
    "                    # update resolved_map ({discarded_id: kept_id}) for this group  \n",
    "                    for row in group:\n",
    "                        resolved_map[row] = most_recent_row_id\n",
    "\n",
    "                    # index to keep in original df\n",
    "                    # gdf.index.get_loc(gdf[gdf['fsq_place_id'] == most_recent_row_id].index[0])\n",
    "                    most_recent_label_index = gdf[gdf['fsq_place_id'] == most_recent_row_id].index[0]\n",
    "                    # print(\"most_recent_label_index: \" + str(most_recent_label_index))\n",
    "                    print(\"row name at that index: \" + gdf.at[most_recent_label_index, 'name'])\n",
    "\n",
    "                    # update row to keep in original gdf\n",
    "                    gdf.at[most_recent_label_index, 'name'] = common_name\n",
    "                    print(\"common_name: \"+ common_name)\n",
    "                    if not all(not d for d in rows_in_group['date_closed']):\n",
    "                        gdf.at[most_recent_label_index, 'date_closed'] = rows_in_group['date_closed'].sort_values(ascending = False).iloc[0]\n",
    "\n",
    "                    # update gdf and df with the common name and centroid\n",
    "                    gdf.at[most_recent_label_index, 'latitude'] = latitude\n",
    "                    gdf.at[most_recent_label_index, 'longitude'] = longitude\n",
    "                    gdf.at[most_recent_label_index, 'geometry'] = centroid\n",
    "                    # print(\"most_recent_row_id: \" + most_recent_row_id)\n",
    "                    drop_ids = set(group) - {most_recent_row_id}\n",
    "                    gdf = gdf[~gdf['fsq_place_id'].isin(drop_ids)]\n",
    "                    # print(\"df size: \" + str(len(gdf)))\n",
    "        # print(\"Processed a category\")\n",
    "    if file_path:\n",
    "        metrics_df = pd.DataFrame(all_group_metrics)\n",
    "        metrics_df.to_csv(file_path, index=False)\n",
    "    return gdf, resolved_map\n",
    "\n",
    "async def deduplicate(gdf, max_distance = 50, name_similarity_threshold = 80, precision = 7, blacklist = NYC_BLACKLIST, file_name = None): \n",
    "    \"\"\"\n",
    "    Deduplicates POIs using a grid-search technique, filtering out duplicates based on spatial proximity and fuzzy name similarity.\n",
    "    \n",
    "    Parameters:\n",
    "        gdf (gpd.GeoDataFrame): FSQ GeoDataFrame with 'fsq_place_id', 'name', 'fsq_category_labels', 'latitude', 'longitude', date and 'geometry' columns, and more.\n",
    "        max_distance (float): Maximum distance in meters for considering POIs as duplicates.\n",
    "        name_similarity_threshold (int): Minimum fuzzy match ratio to consider names as duplicates.\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: Deduplicated POIs.\n",
    "        original_saves: a copy of gdf with two additional columns: 'isdup' and 'resolved_fsq_id'. For each POI p, isdup is True if it is a duplicate \n",
    "        and resolved_fsq_id is the fsq_place_id of the kept POI in p's duplicate group. If p is not a duplicate, isdup is False and reoslved_fsq_id is None.\n",
    "    \"\"\"\n",
    "    if gdf.empty:\n",
    "        print(\"At least one input DataFrame is empty. Returning original df and gdf DataFrames.\")\n",
    "        return gdf\n",
    "\n",
    "    gdf = gdf.to_crs(\"EPSG:3857\")\n",
    "\n",
    "    gdf.loc[:, 'date_created'] = pd.to_datetime(gdf['date_created'], errors='coerce')\n",
    "    gdf.loc[:, 'date_closed'] = pd.to_datetime(gdf['date_closed'], errors='coerce')\n",
    "    gdf.loc[:, 'date_refreshed'] = pd.to_datetime(gdf['date_refreshed'], errors='coerce')\n",
    "\n",
    "    gdf = assign_geohashes(gdf, precision)\n",
    "    gdf['parent_id'] = ''\n",
    "    original = gdf.copy()\n",
    "    \n",
    "    gdf_list = []\n",
    "    resolved_map = {}\n",
    "    for hash in gdf['geohash'].unique():\n",
    "        # Filter the DataFrame for the current geohash\n",
    "        gdf_geohash = gdf[gdf['geohash'] == hash]\n",
    "        if gdf_geohash.empty:\n",
    "            continue\n",
    "        \n",
    "        # Get unique categories in the current geohash\n",
    "        gdf_geohash['top_category'] = gdf_geohash['fsq_category_labels'].apply(extract_top_category)\n",
    "\n",
    "        unique_top_categories = gdf_geohash['top_category'].unique().tolist()\n",
    "        # Process groups for the current geohash and its categories\n",
    "        processed_gdf, resolved_map = await process_groups(gdf_geohash, hash, unique_top_categories, name_similarity_threshold, max_distance, precision, blacklist, resolved_map, file_name)\n",
    "        gdf_list.append(processed_gdf)\n",
    "\n",
    "    gdf_concat = pd.concat(gdf_list, ignore_index=True)\n",
    "    gdf = gpd.GeoDataFrame(gdf_concat, geometry='geometry', crs=\"EPSG:3857\")\n",
    "    original_saved = save_results_to_gdf(original, resolved_map)\n",
    "    return gdf, original_saved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f70430",
   "metadata": {},
   "source": [
    "### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ne_df = pd.read_parquet(\"/share/garg/accessgaps2024/fsq_dedup_pipeline/data/fsq_data/ne_fsq_pois_by_region.parquet\")\n",
    "# ma_df = ne_df[(ne_df['latitude'].notna()) & (ne_df['longitude'].notna()) & (ne_df['region'] == \"MA\")]\n",
    "# print(\"ma len: \" + str(len(ma_df)))\n",
    "\n",
    "# MIN_LAT = 42.2279\n",
    "# MAX_LAT = 42.4008\n",
    "# MIN_LON = -71.1912\n",
    "# MAX_LON = -70.9860\n",
    "\n",
    "# boston_df = ma_df[(ma_df['latitude'] >= MIN_LAT) & (ma_df['latitude'] <= MAX_LAT) & (ma_df['longitude'] >= MIN_LON) & (ma_df['longitude'] <= MAX_LON)]\n",
    "# print(\"bos len: \" + str(len(boston_df)))\n",
    "\n",
    "# ri_df = ne_df[(ne_df['latitude'].notna()) & (ne_df['longitude'].notna()) & (ne_df['region'] == \"RI\")]\n",
    "# print(\"ri len: \" + str(len(ri_df)))\n",
    "\n",
    "# # boston_df = boston_df[boston_df['locality'] == 'Belmont']\n",
    "# boston_df['geometry'] = boston_df.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "# boston_gdf = gpd.GeoDataFrame(boston_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "# boston_gdf['date_created'] = pd.to_datetime(boston_gdf['date_created'], errors='coerce')\n",
    "# boston_gdf['date_closed'] = pd.to_datetime(boston_gdf['date_closed'], errors='coerce')\n",
    "\n",
    "# boston_blacklist = []\n",
    "\n",
    "# with open(\"/share/garg/accessgaps2024/fsq_dedup_pipeline/data/blacklists/boston_global_blacklist\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     for line in file:\n",
    "#         words = line.strip().split()\n",
    "#         if words:  # make sure the line isn't empty\n",
    "#             boston_blacklist.append(words[0])\n",
    "# print(boston_blacklist)\n",
    "\n",
    "# boston_dedup, boston_lbled = await deduplicate(boston_gdf, max_distance = 100, name_similarity_threshold = 90, precision = 7, blacklist = boston_blacklist, file_name = '/share/garg/accessgaps2024/fsq_dedup_pipeline/results/childcare_test')\n",
    "\n",
    "# boston_dedup.to_csv(\"/share/garg/accessgaps2024/fsq_dedup_pipeline/results/bos_dedup_pois\")\n",
    "# boston_lbled.to_csv(\"/share/garg/accessgaps2024/fsq_dedup_pipeline/results/bos_lbled_pois\")\n",
    "# dups = boston_lbled[boston_lbled['isdup'] == True].sort_values('name')\n",
    "# # groups = dups.groupby(\"resolved_fsq_id\")[['name']].transform(lambda x: \", \".join(x)).reset_index()\n",
    "# groups = dups.groupby(\"resolved_fsq_id\")[['name']].agg(lambda x: \", \".join(sorted(x))).reset_index()\n",
    "# for i, row in groups.iterrows():\n",
    "#     print(row['name'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wildfires)",
   "language": "python",
   "name": "wildfires"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
