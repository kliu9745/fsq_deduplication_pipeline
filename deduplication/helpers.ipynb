{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd048cc",
   "metadata": {},
   "source": [
    "# All Deduplication Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29af497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import geohash\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c189c82d",
   "metadata": {},
   "source": [
    "### NYC Blacklist + Other Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7ffb43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_FOOD = {\"pizza\", \"pizzeria\", \"cafe\", \"caffe\", \"coffee\", \"grill\", \"restaurant\", \"bar\", \"deli\", \"bakery\", \"express\",\n",
    "                  \"market\", \"shop\", \"mart\", \"store\", \"grocery\", \"supermarket\", \"food\", \"gourmet\", \"cart\", \"fresh\", \"kitchen\", \n",
    "                  \"diner\", \"pub\", \"bistro\", \"tavern\", \"farm\", \"chicken\", \"burger\", \"sandwich\", \"taco\", \"tacos\", \"sushi\", \"noodle\", \"noodles\"\n",
    "                  \"salad\", \"sub\", \"ice cream\", \"dessert\", \"breakfast\", \"brunch\", \"lunch\", \"dinner\", \"takeout\", \"delivery\", \n",
    "                  \"snack\", \"chocolate\", \"tea\", \"juice\", \"smoothie\", \"wine\", \"beer\", \"cocktail\", \"brewery\", \"distillery\", \n",
    "                  \"winery\", \"patisserie\", \"pastry\", \"bagel\", \"donut\", \"pancake\", \"waffle\", \"crepe\", \"treat\", \"cuisine\", \"truck\",\n",
    "                  \"italian\", \"mexican\", \"chinese\", \"japanese\", \"korean\", \"indian\", \"thai\", \"vietnamese\", \"greek\", \"spanish\",\n",
    "                  \"french\", \"american\", \"cuban\", \"cajun\", \"creole\", \"soul food\", \"bbq\", \"steakhouse\", \"seafood\",\n",
    "                  \"vegetarian\", \"vegan\", \"gluten-free\", \"organic\", \"local\", \"artisan\", \"handmade\", \"craft\", \"homemade\",\n",
    "                  \"family-owned\", \"authentic\", \"traditional\", \"fusion\", \"gastro\", \"gastropub\", \"halal\", \"kosher\"}\n",
    "GLOBAL_TRANS_ADDR = {\"east\", \"west\", \"north\", \"south\", \"st\", \"street\", \"ave\", \"avenue\", \n",
    "                 \"blvd\", \"road\", \"rd\", \"rd.\", \"drive\", \"cab\", \"car\", \"truck\", \"van\", \"taxi\", \"metro\", \"sub\", \"subway\", \"mta\", \"station\", \"apt\", \"apartment\", \"station\", \"corner\", \"bus\", \"express\", \"line\", \"line\", \"plaza\", \"plz\", \"square\", \"sq\", \"lane\", \"ln\", \"way\", \"wy\", \"court\", \"ct\",\n",
    "                 \"park\", \"pl\", \"pkwy\", \"parkway\", \"circle\", \"cir\", \"highway\", \"hwy\", \"route\", \"rte\", \"exit\", \"exit\", \"bridge\", \"bridges\", \"crossing\", \"crossings\",\n",
    "                 \"crossings\", \"intersection\", \"intersections\", \"boulevard\", \"boulevards\", \"roadway\", \"roadways\", \"driveway\", \"driveways\",\"avenue\", \"avenues\", \"streetway\"}\n",
    "GLOBAL_LOC = {\"city\", \"village\", \"town\", \"museum\", \"group\", \"house\", \"center\", \"ctr\", \"art\", \"shop\", \"show\", \"theatre\", \"theater\", \"office\", \"service\", \"services\", \"bank\", \"jewelry\", \"club\", \n",
    "              \"community\", \"garden\", \"park\", \"field\", \"beach\", \"ocean\", \"river\", \"playground\", \"school\", \"college\", \"university\", \"library\", \"gallery\", \"studio\", \"hall\", \"auditorium\", \"venue\", \"church\"\n",
    "              \"cleaner\", \"cleaners\", \"laundry\", \"laundromt\", \"pharmacy\", \"church\", \"clinic\", \"gym\", \"hospital\", \"fitness\", \"nail\", \"nails\", \"salon\", \"spa\", \"barber\", \"project\", \"projects\"}\n",
    "GLOBAL_WORDS = {\"the\", \"a\", \"an\", \"and\", \"&\", \"of\", \"in\", \"for\", \"to\", \"at\", \"@\", \"on\", \"out\", \"with\", \"by\", \"from\", \"as\", \"that\", \"this\", \"it\", \"is\", \"was\", \"be\", \"are\", \"day\", \"care\", \"co.\"}\n",
    "GLOBAL_SCHOOL_WRDS = {\"high\", \"middle\", \"elementary\", \"school\", \"academy\", \"charter\", \"magnet\", \"daycare\", \"day\", \"care\"}\n",
    "GLOBAL_FIRE_STATION = {\"fdny\", \"engine\", \"ems\", \"rescue\", \"group\"}\n",
    "GLOBAL_POLICE_STATION = {\"nypd\"}\n",
    "LOCAL_PHRASES_NYC = {\"new\", \"york\", \"nyc\", \"manhattan\", \"brooklyn\", \"queens\", \"bronx\", \"staten\", \"island\", \"ny\", \"city\", \"upper\", \"lower\", \"side\", \"marks\"}\n",
    "COMMON_PHRASES = GLOBAL_FOOD.union(GLOBAL_TRANS_ADDR).union(GLOBAL_LOC).union(GLOBAL_WORDS).union(LOCAL_PHRASES_NYC).union(GLOBAL_SCHOOL_WRDS).union(GLOBAL_FIRE_STATION).union(GLOBAL_POLICE_STATION)\n",
    "\n",
    "# # Neighborhood and local area terms to ignore during name normalization\n",
    "LOCAL_AREAS_NYC = [\n",
    "\n",
    "    # Lower Manhattan / East Side\n",
    "    \"alphabet\", \"lower\", \"west\", \"north\", \"east\", \"south\", \"street\", \"side\", \"les\",\n",
    "    \"two\", \"bridges\", \"chinatown\", \"nolita\", \"soho\", \"noho\", \"little\", \"italy\",\n",
    "    \"bowery\", \"seaport\", \"civic\", \"center\", \"marks\", \"wall\", \"financial\", \"tribeca\", \"fidi\", \"delancey\",\n",
    "    \"clinton\", \"canal\",\n",
    "    \n",
    "    # Midtown / Gramercy / Chelsea\n",
    "    \"gramercy\", \"flatiron\", \"murray\", \"midtown\",\n",
    "    \"koreatown\", \"garment\", \"district\", \"nomad\", \"chelsea\", \"hell\", \"hells\",\n",
    "    \"hudson\", \"yards\", \"theater\", \"times\", \"square\", \"rockefeller\", \"kips\", \"turtle\", \"bay\", \"herald\", \"penn\", \"station\", \"empire\",\n",
    "    \n",
    "    # Upper Manhattan\n",
    "    \"upper\", \"harlem\", \"spanish\", \"heights\",\n",
    "    \"morningside\", \"hamilton\", \"inwood\", \"washington\",\"manhattenville\", \"sugar\", \"hill\", \"dyckman\", \"fort\", \"george\", \"columbia\",\n",
    "    \n",
    "    # Downtown / River Areas / Parks\n",
    "    \"tompkins\", \"river\", \"park\",\n",
    "    \"stuytown\", \"stuyvesant\", \"oval\", \"union\",\n",
    "    \"madison\", \"bryant\", \"central\", \"battery\", \"riverside\", \"fdr\", \"drive\",\n",
    "    \n",
    "    # Outer Boroughs or Bordering Areas\n",
    "    \"brooklyn\", \"queens\", \"bushwick\", \"greenpoint\", \"williamsburg\", \"bed-stuy\",\n",
    "    \"dumbo\", \"long island city\", \"astoria\", \"ridgewood\",\n",
    "]\n",
    "\n",
    "NYC_BLACKLIST = COMMON_PHRASES.union(LOCAL_AREAS_NYC)\n",
    "\n",
    "# Simple number word mappings up to 100 (expand if needed)\n",
    "NUM_WORDS = {\n",
    "    \"zero\": 0, \"one\": 1, \"first\": 1,\n",
    "    \"two\": 2, \"second\": 2,\n",
    "    \"three\": 3, \"third\": 3,\n",
    "    \"four\": 4, \"fourth\": 4,\n",
    "    \"five\": 5, \"fifth\": 5,\n",
    "    \"six\": 6, \"sixth\": 6,\n",
    "    \"seven\": 7, \"seventh\": 7,\n",
    "    \"eight\": 8, \"eighth\": 8,\n",
    "    \"nine\": 9, \"ninth\": 9,\n",
    "    \"ten\": 10, \"tenth\": 10,\n",
    "    \"eleven\": 11, \"eleventh\": 11,\n",
    "    \"twelve\": 12, \"twelfth\": 12,\n",
    "    \"thirteen\": 13, \"thirteenth\": 13,\n",
    "    \"fourteen\": 14, \"fourteenth\": 14,\n",
    "    \"fifteen\": 15, \"fifteenth\": 15,\n",
    "    \"sixteen\": 16, \"sixteenth\": 16,\n",
    "    \"seventeen\": 17, \"seventeenth\": 17,\n",
    "    \"eighteen\": 18, \"eighteenth\": 18,\n",
    "    \"nineteen\": 19, \"nineteenth\": 19,\n",
    "    \"twenty\": 20, \"twentieth\": 20,\n",
    "    \"thirty\": 30, \"thirtieth\": 30,\n",
    "    \"forty\": 40, \"fortieth\": 40,\n",
    "    \"fifty\": 50, \"fiftieth\": 50,\n",
    "    \"sixty\": 60, \"sixtieth\": 60,\n",
    "    \"seventy\": 70, \"seventieth\": 70,\n",
    "    \"eighty\": 80, \"eightieth\": 80,\n",
    "    \"ninety\": 90, \"ninetieth\": 90\n",
    "}\n",
    "\n",
    "# Normalize street suffixes\n",
    "SUFFIX_MAP = {\n",
    "    \"st\": \"street\", \"st.\": \"street\", \"street\": \"street\",\n",
    "    \"rd\": \"road\", \"rd.\": \"road\", \"road\": \"road\",\n",
    "    \"ave\": \"avenue\", \"ave.\": \"avenue\", \"avenue\": \"avenue\",\n",
    "    \"blvd\": \"boulevard\", \"blvd.\": \"boulevard\", \"boulevard\": \"boulevard\",\n",
    "    \"ln\": \"lane\", \"ln.\": \"lane\", \"lane\": \"lane\",\n",
    "    \"dr\": \"drive\", \"dr.\": \"drive\", \"drive\": \"drive\",\n",
    "    \"ct\": \"court\", \"ct.\": \"court\", \"court\": \"court\",\n",
    "    \"pl\": \"place\", \"pl.\": \"place\", \"place\": \"place\"\n",
    "}\n",
    "def normalize_suffix(word):\n",
    "    return SUFFIX_MAP.get(word.lower(), word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044bf490",
   "metadata": {},
   "source": [
    "### FSQ Deduplication Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ceb41",
   "metadata": {},
   "source": [
    "Geohash Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad6f8446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_geohashes(df, precision=7):\n",
    "    \"\"\"\n",
    "    Creates a new column, 'geohash', in [df] with the geohash code for each POI row.\n",
    "    \n",
    "    The length of the geohash code is [precision]. Longer geohashes are more precise and thus are smaller regions.\n",
    "    Precision 1 is a ~(5000km x 5000km) area (ex: large country) while Precision 7 is a ~(153m x 153m) area (ex: size of a Manhattan zip).\n",
    "\n",
    "    df (FSQ DataFrame/GeoDataFrame): must have 'latitude' and 'longitude' columns\n",
    "    precision: length of a geohash -> the size of each grid in the partition. 1 <= precision <= 12.\n",
    "    \"\"\"\n",
    "    assert 'latitude' in df.columns and 'longitude' in df.columns\n",
    "    assert precision >= 1 and precision <= 12\n",
    "\n",
    "    df['geohash'] = df.apply(lambda row: geohash.encode(row['latitude'], row['longitude'], precision=precision), axis=1)\n",
    "    return df\n",
    "\n",
    "def get_neighboring_geohashes(hash):\n",
    "    \"\"\" \n",
    "    Returns a list of [hash] and the geohash codes of [hash]'s eight immediate neighbors.\n",
    "    Can be used to query surrounding areas for a POI or nearby POIs.\n",
    "\n",
    "    Parameters:\n",
    "    hash (string): geohash between length 1-12\n",
    "    \"\"\"\n",
    "    assert len(hash) >= 1 and len(hash) <= 12\n",
    "    neighbors = geohash.neighbors(hash)\n",
    "    # include the original geohash\n",
    "    return [hash] + neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb7d5e6",
   "metadata": {},
   "source": [
    "Name Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cde9c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_number(tokens):\n",
    "    \"\"\"Convert tokens from their word form to digit-string form. \n",
    "    Ex: \n",
    "    words_to_number(['twenty', 'second']) = ['22']\n",
    "    words_to_number(['seven'] = ['7'])\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        t = tokens[i]\n",
    "        if t in NUM_WORDS:\n",
    "            val = NUM_WORDS[t]\n",
    "            j = i + 1\n",
    "            if j < len(tokens) and tokens[j] in NUM_WORDS:\n",
    "                comb = val + NUM_WORDS[tokens[j]]\n",
    "                result.append(str(comb))\n",
    "                i += 2\n",
    "                continue\n",
    "            result.append(str(val))\n",
    "        else:\n",
    "            result.append(t)\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "def capitalize_str(name):\n",
    "    \"\"\" \n",
    "    Capitalizes the first letter of each word in [name].\n",
    "\n",
    "    Parameters:\n",
    "        name (str): POI name\n",
    "    \"\"\"\n",
    "    parts = name.split()\n",
    "    upper = [p.capitalize() for p in parts]\n",
    "    return \" \".join(upper)\n",
    "\n",
    "def clean_name(name, lower = False):\n",
    "    \"\"\" \n",
    "    Cleans a name by removing excess spaces and accents, and replacing punctuation/symbols with a space.\n",
    "\n",
    "    Parameters:\n",
    "    name (str): a POI location name\n",
    "    lower (boolean): whether to make the name all lowercase or not.\n",
    "    \"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        return \"\"\n",
    "    # Collapse multiple spaces\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    # Remove accents/diacritics (e.g. é → e)\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode()\n",
    "\n",
    "    # Replace punctuation/symbols with space\n",
    "    name = re.sub(r'[^a-zA-Z0-9\\s]', ' ', name)\n",
    "    return name.lower() if lower else name\n",
    "\n",
    "def remove_common_words(name, blacklist = NYC_BLACKLIST):\n",
    "    \"\"\"\n",
    "    Removes ordinal suffixes (letters attached to digits) from [name],\n",
    "    converts all chars into lowercase and word numbers into their digit forms. \n",
    "    Then, filters out any tokens in [blacklist], all digits, and 1-letter words.\n",
    "\n",
    "    Parameters:\n",
    "        name (str): FSQ point of interest name\n",
    "        blacklist (str list, optional): list of words to filter out of name \n",
    "        (common filler words, apt/building numbers, directions like n/w/s/e)\n",
    "    \"\"\"\n",
    "    name = re.sub(r'\\b(\\d+)(st|nd|rd|th)\\b', r'\\1', name)\n",
    "    parts = name.lower().split()\n",
    "    parts = words_to_number(parts)\n",
    "    parts = [p for p in parts if p not in blacklist]\n",
    "    return \" \".join(p for p in parts if not p.isdigit() and len(p) > 1)\n",
    "\n",
    "def longest_common_substring(strs, min_len = 3, blacklist = NYC_BLACKLIST):\n",
    "    \"\"\"\n",
    "    Find the longest common substring between a list of strings.\n",
    "    Parameters:\n",
    "        strs (list): List of 2+ strings to compare.\n",
    "        min_len (int, optional): minimum length allowed for of a longest common substring\n",
    "        blacklist (list, optional): words to remove from [strs] to form the common substring\n",
    "    \"\"\"\n",
    "    assert strs is not None and len(strs) >= 2, \"At least two strings are required to find a common substring.\"\n",
    "    strs = [remove_common_words(clean_name(s, True)) for s in strs]\n",
    "    shortest_str = min(strs, key = len)\n",
    "    longest_substring = \"\"\n",
    "\n",
    "    n = len(shortest_str)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n + 1):\n",
    "            substr = shortest_str[i:j]\n",
    "            if all(substr in s for s in strs) and len(substr.strip()) >= min_len:\n",
    "                if len(substr) > len(longest_substring):\n",
    "                    longest_substring = substr.strip()\n",
    "    return longest_substring\n",
    "\n",
    "\n",
    "def choose_common_name_from_group(names, blacklist = NYC_BLACKLIST):\n",
    "    \"\"\" \n",
    "    Returns the representative name amongst a group of [names].\n",
    "    If there exists at least one name that equals the longest common substring of the group,\n",
    "    then returns the substring. \n",
    "    Otherwise, selects the first occurrence of the most frequent name in the group (using Counter).\n",
    "\n",
    "    If the longest substring is less than 3 chars or is fully in [blacklist] it returns None.\n",
    "    If all names are addresses with the same street but different numbers, it returns None.\n",
    "\n",
    "    Parameters:\n",
    "        names (str list): group of names, must not be None\n",
    "        blacklist (str list): words to be filtered out and not used to determine the representative name \n",
    "    \"\"\"\n",
    "    assert names\n",
    "    cleaned_names = [clean_name(n) for n in names]\n",
    "    longest_substr = longest_common_substring(names)\n",
    "    if len(longest_substr) < 3 or longest_substr.lower() in blacklist:\n",
    "        return None\n",
    "    \n",
    "    # Compute address parts\n",
    "    addr_parts = []\n",
    "    for n in cleaned_names:\n",
    "        parts = n.split()\n",
    "        if len(parts) > 1 and parts[0].isdigit():\n",
    "            num = parts[0]\n",
    "            # street_tokens = [normalize_suffix(p) for p in parts[1:]]\n",
    "            # street = \" \".join(street_tokens)\n",
    "            street = parts[1].lower()\n",
    "            if street and num:\n",
    "                addr_parts.append((num, street))\n",
    "        # elif len(parts) > 1 and parts[-1].isdigit():\n",
    "        #     num = parts[-1]\n",
    "        #     prefix = parts[-2]\n",
    "        #     if prefix and num:\n",
    "        #         addr_parts.append((num, prefix))\n",
    "\n",
    "    street_set = {s for _, s in addr_parts}\n",
    "    number_set = {n for n,_ in addr_parts}\n",
    "\n",
    "    # print(\"street_set: \" + str(street_set))\n",
    "    # print(\"num_set: \" + str(number_set))\n",
    "    if len(street_set) == 1 and len(number_set) > 1:\n",
    "        return None\n",
    "\n",
    "    for og_name, cleaned_name in zip(names, cleaned_names):\n",
    "        bklst_filtered_name = remove_common_words(cleaned_name, blacklist)\n",
    "        if bklst_filtered_name == longest_substr.lower():\n",
    "            return og_name\n",
    "            # return capitalize_str(og_name)\n",
    "    return Counter(names).most_common(1)[0][0]\n",
    "    # return capitalize_str(Counter(names).most_common(1)[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba61cbce",
   "metadata": {},
   "source": [
    "Other Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ea0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fsq_csv_to_gdf(fsq_file, geometry_col ='geometry'):\n",
    "    \"\"\" \n",
    "    Loads a FSQ Places dataset (CSV or Parquet) and converts it into a GeoDataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        fsq_file (str): path to FSQ Dataset file (CSV or Parquet)\n",
    "        geometry_col (str): column name used to store POI geometries\n",
    "    \"\"\"\n",
    "    assert isinstance(fsq_file, str)\n",
    "    try:\n",
    "        df = pd.read_csv(fsq_file)\n",
    "    except:\n",
    "        df = pd.read_parquet(fsq_file)\n",
    "    df[geometry_col] = df.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry_col, crs=\"EPSG:4326\")\n",
    "    gdf['date_created'] = pd.to_datetime(gdf['date_created'], errors='coerce')\n",
    "    gdf['date_closed'] = pd.to_datetime(gdf['date_closed'], errors='coerce')\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def extract_top_category(cat_array):\n",
    "    \"\"\"\n",
    "    Returns the top-level category of a single 'fsq_category_label' entry within a POI row.\n",
    "    If cat_array is not an np.ndarray or is an empty np.ndarray, then returns ''\n",
    "\n",
    "    Parameters:\n",
    "        cat_array (np.ndarray): array of string category labels, where each label is broken into hierarchical subcategories using '<'/\n",
    "    Returns: a string representing the top-level category of a label\n",
    "    \"\"\"\n",
    "    if isinstance(cat_array, np.ndarray) and len(cat_array) > 0:\n",
    "        return cat_array[0].split(' > ')[0].strip()\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def filter_by_zcta(gdf, zcta, zcta_codes):\n",
    "    \"\"\"\n",
    "    Filters the GeoDataFrame [gdf] to include only POIs within the specified ZCTA code.\n",
    "    \n",
    "    Parameters:\n",
    "        gdf (gpd.GeoDataFrame): GeoDataFrame containing POIs with 'geometry' column.\n",
    "        zcta (gpd.GeoDataFrame): GeoDataFrame containing ZCTA polygons.\n",
    "        zcta_code (str): The ZCTA code to filter by.\n",
    "    \n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: Filtered GeoDataFrame containing only POIs within the specified ZCTA.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure both CRS's are EPSG:4326 (meters)\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "    zcta = zcta.to_crs(epsg=4326)\n",
    "    zcta_filtered = zcta[zcta['ZCTA5CE20'].isin(zcta_codes)]\n",
    "    return gpd.sjoin(gdf, zcta_filtered, predicate = 'within')\n",
    "\n",
    "def select_most_recent_row(df):\n",
    "    \"\"\"\n",
    "    Take the most recent row from a group of sPOIs based on 'date_refreshed' or 'date_created'.\n",
    "    If 'date_refreshed' is NaN for all rows, use 'date_created' instead.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing a group of POIs with 'date_refreshed' and 'date_created' columns.\n",
    "    \"\"\"\n",
    "    if df['date_refreshed'].notna().any():\n",
    "        return df.sort_values(by = 'date_refreshed', ascending=False).iloc[0]\n",
    "    else:\n",
    "        return df.sort_values(by = 'date_created', ascending=False).iloc[0]\n",
    "    \n",
    "def normalize_group(df, cols):\n",
    "    \"\"\" \n",
    "    Standardizes each column in [cols] in [df] and stores it in a new column, '[col]_norm', in [df].\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): must have at least one column\n",
    "        cols (str list): list of column names in [df]\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        assert col in df.columns\n",
    "        scaler = MinMaxScaler()\n",
    "        df[col + \"_norm\"] = scaler.fit_transform(df[col])\n",
    "    return df\n",
    "\n",
    "def save_results_to_gdf(df, resolved_map):\n",
    "    \"\"\"\n",
    "    Adds two new columns to [df]: 'isdup' and 'resolved_fsq_id'.\n",
    "    For each row in [df], 'isdup' = True when this row is a duplicate and isdup = 0 otherwise\n",
    "    For each duplicate row r in [df], 'resolved_fsq_id' maps r to its corr. row in the filtered df, that it was merged into.\n",
    "    Parameters:\n",
    "    df:\n",
    "    dup_lst:\n",
    "    \"\"\"\n",
    "    df['isdup'] = df['fsq_place_id'].isin(resolved_map.keys()).astype(bool)\n",
    "    df['resolved_fsq_id'] = df['fsq_place_id'].map(resolved_map)\n",
    "    # update for kept rows in filtered df\n",
    "    kept_ids = set(resolved_map.values())\n",
    "    kept = df['fsq_place_id'].isin(kept_ids)\n",
    "    df.loc[kept, 'isdup'] = True\n",
    "    df.loc[kept, 'resolved_fsq_id'] = df.loc[kept, 'fsq_place_id']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2373d3f",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afce3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "135 Shillman Hall\n",
      "64 Revere St\n",
      "Hodan Management\n",
      "Star Market\n",
      "Aldrich 007\n",
      "1330 Boylston\n",
      "WordCamp Boston 2012\n",
      "Lecture Hall 505\n"
     ]
    }
   ],
   "source": [
    "# not duplicates\n",
    "print(choose_common_name_from_group([\"21 Cameron Ave\", \"38 cameron ave\", \"45 Cameron Ave\"]))\n",
    "print(choose_common_name_from_group(['77 Summer Street', '87 Summer Street']))\n",
    "print(choose_common_name_from_group([\"20 Hemenway\", \"38 Hemenway Apts\", \"39 hemenway\", \"45 Hemenway Street\",\n",
    "                                      \"Fenway Dorm 12 Hemenway St.\"]))\n",
    "print(choose_common_name_from_group(['500 Boylston St', '501 Boylston St']))\n",
    "print(choose_common_name_from_group(['12 Wendell', '14 Wendell']))\n",
    "print(choose_common_name_from_group(['211 Newbury', '215 Newbury Street', 'Prince of Newbury Street']))\n",
    "print(choose_common_name_from_group(['1778 Commonwealth', '1800 Commonwealth Ave']))\n",
    "print(choose_common_name_from_group(['156 Pleasant St', '157 Pleasant']))\n",
    "print(choose_common_name_from_group(['156 Pleasant Street', '157 Pleasant St.']))\n",
    "print(choose_common_name_from_group(['41 Revere St', '70 Revere St Roof Top']))\n",
    "\n",
    "# duplicates\n",
    "print(choose_common_name_from_group(['135 Shillman Hall', 'Shillman Hall']))\n",
    "print(choose_common_name_from_group(['64 Revere St', 'revere st']))\n",
    "print(choose_common_name_from_group(['Hodan Management', 'Hodan Property Management & Dev']))\n",
    "print(choose_common_name_from_group(['Flower Market At Star Market Quincy', 'Star Market', 'Stars at Quincy']))\n",
    "print(choose_common_name_from_group([\"Aldrich 007\", \"Aldrich 008\", \"Aldrich 010\", \"Aldrich 011\", \"Aldrich 107\", \n",
    "\"Aldrich 109\", \"Aldrich 110\", \"Aldrich 111\", \"Aldrich 209\", \"Aldrich 211\", \"Aldrich 9\", \"Aldrich Hall\"\n",
    "]))\n",
    "print(choose_common_name_from_group(['1330 Boylston', '1330 Boylston - Luxury Apartments']))\n",
    "print(choose_common_name_from_group(['WordCamp Boston 2012', 'Wordcamp Boston 2011']))\n",
    "print(choose_common_name_from_group(['Lecture Hall 505', 'Lecture Hall 511']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wildfires)",
   "language": "python",
   "name": "wildfires"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
